{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas  (from <a href=\"https://github.com/noise42/datastructures/blob/master/materials/python-data-science-handbook.pdf\">here</a> and <a href=\"https://python4bioinformaticsblog.wordpress.com/index/python-bits/pandas/\">here</a>)\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is a package built on top of NumPy, and provides an efficient implementation of a ``DataFrame``. ``DataFrame``s are essentially multidimensional arrays with attached row and column labels, and often with heterogeneous types and/or missing data.\n",
    "\n",
    "### The Pandas Series Object\n",
    "A Pandas ``Series`` is a one-dimensional array of indexed data. It can be created from a list or array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.Series(['RNA', 'gene', 'protein'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Series`` wraps both a sequence of values and a sequence of indices, which we can access with the ``values`` and ``index`` attributes. The ``values`` are simply a NumPy array, while the ``index`` is an array-like object of type ``pd.Index``. Note that the index need not be an integer, but can consist of values of any desired type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(\n",
    "    ['RNA', 'gene', 'protein'],\n",
    "    index=['ENSG', 'ENSP', 'ENST']\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part, the `dtype`, is not part of the elements you access when cycling through the values of the ``Series``, it is just another attribute.\n",
    "\n",
    "We can construct a ``Series`` from a dictionary and the way we access the values are similar to dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {'ENST': 'RNA', 'ENSG': 'gene', 'ENSP': 'protein'}\n",
    "data = pd.Series(map_dict)\n",
    "\n",
    "data['ENSG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Series`` support slicing just like other arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ENSG':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pandas DataFrame Object\n",
    "The ``DataFrame`` can be thought as a generalization of a mix of both a NumPy array and a dictionary. It can be constructed from 2 or more dictionary with the same keys (or from 2 ``Series`` with the same indexes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {'ENST': 3300, 'ENSG': 18435, 'ENSP': 12034}\n",
    "groups_dict = {'ENST': 13, 'ENSG': 42, 'ENSP': 157}\n",
    " \n",
    "df = pd.DataFrame({'mapping type': map_dict, 'counts': count_dict, 'classes': groups_dict})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the index labels with the ``DataFrame`` attribute ``index``. Additionally, the ``DataFrame`` has a ``columns`` attribute, which holds the labels for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index, df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access a colum like a dictionary or in a Pandas way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['counts']  # like a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.counts  # The Pandas way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is that the dictionary way supports labels with spaces and special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mapping type']\n",
    "#df.mapping type  # I can't do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary-style syntax can also be used to modify the object, in this case adding a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['averages'] = df['counts'] / df['classes']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexers: loc, iloc\n",
    "Slicing and indexing conventions can be a source of confusion. For example, if your ``Series`` has an explicit integer index, an indexing operation such as ``data[1]`` will use the explicit indices, while a slicing operation like ``data[1:3]`` will use the implicit Python-style index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\n",
    "\n",
    "print(data[1])   # explicit index when indexing\n",
    "print(data[1:3]) # implicit index when slicing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides some special *indexer* attributes that explicitly expose certain indexing/slicing schemes.\n",
    "\n",
    "The ``loc`` attribute allows indexing and slicing that always references the explicit index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.loc[1])\n",
    "print(data.loc[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``iloc`` attribute allows indexing and slicing that always references the implicit Python-style index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[1])\n",
    "print(data.iloc[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little tip:\n",
    "> # explicit is better than implicit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame as two-dimensional array\n",
    "We can also view the ``DataFrame`` as an enhanced two-dimensional array. We can examine the raw underlying data array using the ``values`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ``iloc`` indexer, we can index the underlying array as if it is a simple NumPy array (using the implicit Python-style index), but the ``DataFrame`` index and column labels are maintained in the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:3, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, using the ``loc`` indexer we can index the underlying data in an array-like style but using the explicit index and column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:'ENSG', :'classes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional indexing conventions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:'ENSG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, direct masking operations are also interpreted row-wise rather than column-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.counts > 5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "To indicate the presence of missing data in a table, Pandas uses a *sentinel value* that indicates a missing entry. In particular, it uses two already-existing Python null values: the special floating-point ``NaN`` value, and the Python ``None`` object.\n",
    "\n",
    "#### ``None``: Pythonic missing data\n",
    "``None`` is a Python singleton object, so it cannot be used in any arbitrary NumPy/Pandas array, but only in arrays with data type ``'object'`` (i.e., arrays of Python objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vals1 = np.array([1, None, 3, 4])\n",
    "vals1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``dtype=object`` means that the best common type representation NumPy could infer for the contents of the array is that they are Python objects. The use of Python objects in an array means that if you perform aggregations like ``sum()`` or ``min()`` across an array with a ``None`` value, you will generally get an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``NaN``: Missing numerical data\n",
    "``NaN`` (acronym for *Not a Number*) is a special floating-point value recognized by all systems that use the standard IEEE floating-point representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2 = np.array([1, np.nan, 3, 4]) \n",
    "vals2.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NumPy chose a native floating-point type for this array, but be aware that the results of arithmetic with ``NaN`` will be another ``NaN``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2.sum(), vals2.min(), vals2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy does provide some special aggregations that will ignore these missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['ENSG', 'counts'] = np.nan\n",
    "df.loc['ENSG', 'classes'] = np.nan\n",
    "df.loc['ENST', 'classes'] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping null values\n",
    "We cannot drop single values from a ``DataFrame``; we can only drop full rows or full columns.\n",
    "\n",
    "By default, ``dropna()`` will drop all rows in which *any* null value is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna() # It returns the modified Dataframe without the 'ENST' and 'ENSG' row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Alternatively, you can drop NA values along a different axis; ``axis=1`` (or ``axis='columns'``) drops all columns containing a null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1)  # It returns the modified Dataframe without the 'counts' and 'classes' columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify ``how='all'``, which will only drop rows/columns that are *all* null values. For finer-grained control, the ``thresh`` parameter lets you specify a minimum number of non-null values for the row/column to be kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1, thresh=2)  # The 'classes' column is dropped because it doensn't have at least 2 non-nul values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute valuesFilling null values\n",
    "Since removing rows can be limiting with some problems, imputing missing values is a valid alternative. The word 'imputing' refers to using a model to replace missing values. For example, we can replace missing data with:\n",
    "\n",
    "- a constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0) # <=> df.replace(np.nan, 0) # The methods return the modified Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a mean, median or mode of the column to the missing data belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean()) # It returns the modified Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from CSV Files\n",
    "CSV (comma-separated value) and TSV (tab-separated value) files are common file formats for transferring and storing data.\n",
    "\n",
    "As an example, we have a file where the values are tab-separated, the first row specifies the column names, and the first column contains the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head brca_transcripts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of files can be load into a Pandas ``DataFrame`` using the ``read_csv`` function in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1_df = pd.read_csv('brca_transcripts.txt', sep = '\\t', index_col = 0, header = 0)\n",
    "brca1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``sep`` specifies the delimiter to use (the tab);\n",
    "- ``index_col`` specifies the column to use as the row labels of the ``DataFrame`` (the first column);\n",
    "- ``header`` specifies the row number to use as the column names (the first row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation and Grouping\n",
    "Pandas ``Series`` and ``DataFrame``s include a method ``describe()`` that computes several common aggregates for each column and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so-called ``groupby`` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(brca1_df.groupby('biotype')))\n",
    "\n",
    "brca1_df.groupby('biotype').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``GroupBy`` object supports column indexing in the same way as the ``DataFrame``, and returns a modified ``GroupBy`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1_df.groupby('biotype')['bp'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply\n",
    "The ``apply()`` method lets you apply a function to the group results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1_df.groupby('biotype')[['bp', 'aa']].apply(np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the ``apply()`` method lets you apply a function along input axis of a ``DataFrame``. Objects passed to these functions are ``Series`` objects having index:\n",
    "- either the ``DataFrame``’s index (``axis=0``)\n",
    "- or the columns (``axis=1``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1_df[['bp', 'aa']].apply(np.sum)            # Total nucleotides and total aminoacids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1_df[['bp', 'aa']].apply(np.sum, axis=1)    # Nucleotides + aminoacids for each transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define an arbitrary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(row, value):\n",
    "    status = ''\n",
    "    if row['bp'] >= value:\n",
    "        status = 'High'\n",
    "    else:\n",
    "        status = 'Low'\n",
    "        \n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the apply requires only one argument. This requirement can be bypassed by \"args\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_mean = brca1_df['bp'].mean()\n",
    "print('bp mean:', bp_mean)\n",
    "\n",
    "brca1_df['transcript_length'] = brca1_df.apply(function, args = (bp_mean,), axis = 1)\n",
    "brca1_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python <strong>lambdas</strong> are little, anonymous functions, subject to a more restrictive but more concise syntax than regular Python functions. Anonymous function means that a function is without a name.\n",
    "\n",
    "The ``def`` keyword is used to define the normal functions and the ``lambda`` keyword is used to create anonymous functions. It has the following syntax: ``lambda arguments: expression``. This function can have any number of arguments but <strong>only one</strong> expression, which is evaluated and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca1_df['protein_length'] = brca1_df.apply(\n",
    "    lambda row, value: 'High' if row['aa'] > value else 'Low', args = (brca1_df['bp'].mean(),),\n",
    "    axis = 1\n",
    ")\n",
    "brca1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that lambda definition does not include a ``return`` statement, it always contains an expression which is returned. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
